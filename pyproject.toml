[project]
name = "prime-rl"
version = "0.4.0"
description = "Async RL Training at Scale"
readme = "README.md"
requires-python = "~=3.12.0"
dependencies = [
    "beartype>=0.21.0",
    "datasets>=4.0.0",
    "jaxtyping>=0.3.2",
    "liger-kernel>=0.5.10",
    "loguru>=0.7.3",
    "pyarrow>=21.0.0",
    "pydantic>=1.10.13",
    "pydantic-settings>=2.12.0",
    "tomli>=2.2.1",
    "tomli-w>=1.2.0",
    "numpy>=2.2.6",
    "torch>=2.9.0",
    "torchdata>=0.11.0",
    "transformers",
    "vllm",
    "wandb>=0.24.2",
    "ring-flash-attn>=0.1.8",
    "prime>=0.5.37",
    "pyzmq>=27.1.0",
    "aiolimiter>=1.2.1",
    "math-env",
    "tenacity>=8.2.0",
    "openai>=1.106.1",
    "rich>=14.0.0",
    "uvloop>=0.21.0",
    "torchtitan",
    "verifiers",
    "dion",
    "reverse-text",
    "tilelang>=0.1.8",
    "arc-agi",
]

[project.scripts]
rl = "prime_rl.entrypoints.rl:main"
sft = "prime_rl.entrypoints.sft:main"
trainer = "prime_rl.trainer.rl.train:main"
orchestrator = "prime_rl.orchestrator.orchestrator:main"
inference = "prime_rl.inference.server:main"
env-server = "prime_rl.orchestrator.env_server.env_server:main"

[project.entry-points."vllm.general_plugins"]
prime_rl = "prime_rl.inference.patches:transformers_v5_compat"

[project.optional-dependencies]
flash-attn = [
    "flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3%2Bcu128torch2.10-cp312-cp312-linux_x86_64.whl",
]
flash-attn-3 = [
    "flash_attn_3 @ https://github.com/samsja/flash-attn-builds/releases/download/v0.1/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl",
]
flash-attn-cute = [
    "flash-attn-cute",
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "ipywidgets>=8.1.7",
    "pre-commit>=4.2.0",
    "pytest>=8.4.1",
    "ruff>=0.12.1",
]


[tool.uv]
preview = true
no-build-isolation-package = ["flash-attn"]
prerelease = "allow"
# Override torch's pinned cuDNN to fix Conv3d performance regression (torch 2.9 + cuDNN 9.8-9.14)
# See: https://github.com/pytorch/pytorch/issues/166122
override-dependencies = [
    "nvidia-cudnn-cu12>=9.15",
    "transformers>=5.1.0.dev0",
]

[tool.uv.sources]
torch = { index = "pytorch-cu128" }
math-env = { index = "primeintellect" }
verifiers = { git = "https://github.com/PrimeIntellect-ai/verifiers.git", rev = "3f555af" }
torchtitan = { git = "https://github.com/pytorch/torchtitan", rev = "a1fdd7e" }
dion = { git = "https://github.com/samsja/dion.git", rev = "d891eeb" }
transformers = { git = "https://github.com/huggingface/transformers.git", rev = "609e3d5" }
vllm = { url = "https://vllm-wheels.s3.us-west-2.amazonaws.com/7a06e5b05b170d7da31845866da0a99fc65253a1/vllm-0.16.0rc3-cp38-abi3-manylinux_2_31_x86_64.whl" }
flash-attn-cute = { git = "https://github.com/Dao-AILab/flash-attention.git", subdirectory = "flash_attn/cute", rev = "main" }
reverse-text = { index = "primeintellect" }
arc-agi = { path = "environments/arc_agi", editable = true }

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]
flash-attn-3 = [{ requirement = "torch", match-runtime = true }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

[[tool.uv.index]]
name = "primeintellect"
url = "https://hub.primeintellect.ai/primeintellect/simple/"

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/test/cu128"
explicit = true

[tool.ruff.lint]
select = ["F", "I"]
ignore = ["F722", "F821"] # Need to ignore for jaxtyping (https://docs.kidger.site/jaxtyping/faq/)

[tool.ruff]
line-length = 120

[tool.pytest.ini_options]
addopts = "--strict-markers"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "gpu: marks tests as gpu (deselect with '-m \"not gpu\"')",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.metadata]
allow-direct-references = true
