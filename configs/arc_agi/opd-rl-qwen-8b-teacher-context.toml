max_steps = 200
seq_len = 65536

[deployment]
num_train_gpus = 1
num_infer_gpus = 2
num_teacher_gpus = 1

[model]
name = "willcb/Qwen3-8B"

[wandb]
project = "arc-agi-opd"
name = "arc-agi-repl-phase1-privileged-teacher"

[ckpt]
interval = 10

# --- Trainer ---
[trainer.model.lora]
rank = 32
alpha = 32
dropout = 0.05
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

[trainer.model.ac]
freq = 1

[trainer.loss]
teacher_tau = 0.5      # Higher tau — privileged teacher provides stronger directional signal
adv_tau = 1.0          # full RL reward signal (hybrid mode)

[trainer.optim]
lr = 5e-6              # 5x increase from 1e-6 — policy was frozen (mismatch_kl=0.0005)
weight_decay = 0.0

# --- Orchestrator ---
[orchestrator]
batch_size = 128
rollouts_per_example = 16
oversampling_factor = 2.0
max_concurrent = 64

[orchestrator.sampling]
temperature = 1.0
max_tokens = 4096

[[orchestrator.env]]
id = "arc-agi"
name = "arc-agi-repl"
args = { dataset_name = "arc-prize-2024", eval_dataset = "arc-prize-2024", eval_split = "evaluation", max_turns = 20}

[orchestrator.eval]
interval = 25           # was 100 — get 8 eval points across 200 steps instead of 2
rollouts_per_example = 4
num_examples = 8

[[orchestrator.eval.env]]
id = "arc-agi"
name = "arc-agi-eval"
args = { dataset_name = "arc-prize-2024", split = "evaluation", max_turns = 20}

[orchestrator.eval.sampling]
temperature = 1.0

[orchestrator.wandb.log_extras]
samples = true
distributions = true
interval = 10

# --- Inference (student: 8B) ---
[inference]
gpu_memory_utilization = 0.90

[inference.model]
name = "willcb/Qwen3-8B"
max_model_len = 65536
dtype = "bfloat16"

# --- Teacher Inference (32B on 1 GPU) ---
# Phase 1: teacher sees ground-truth test outputs via privileged prompt (teacher_context in data)
# No config flag needed — enabled automatically when info["teacher_context"] is present
[teacher_inference]
gpu_memory_utilization = 0.90

[teacher_inference.server]
port = 8032

[teacher_inference.model]
name = "willcb/Qwen3-32B"
max_model_len = 65536
dtype = "bfloat16"
